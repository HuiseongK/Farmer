{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57ba88fb-b70b-4c62-b994-8d9b3eca4f7b",
   "metadata": {},
   "source": [
    "# 패키지 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d455b1-bdfb-4847-af8f-038cd054d34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from types import SimpleNamespace\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from blitz.modules.base_bayesian_module import BayesianModule, BayesianRNN\n",
    "from blitz.modules.weight_sampler import TrainableRandomDistribution, PriorWeightDistribution\n",
    "from blitz.utils import variational_estimator\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125480ef-854f-49f2-b95f-7e2e7a205e37",
   "metadata": {},
   "source": [
    "# 데이터 준비 (베이스라인)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4958a15-f203-4913-b3e9-6de73d9be434",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"epoch\": 30,\n",
    "    \"batch_size\": 64,\n",
    "    \"hidden_size\": 64,\n",
    "    \"num_layers\": 2,\n",
    "    \"output_size\": 3\n",
    "}\n",
    "\n",
    "CFG = SimpleNamespace(**config)\n",
    "\n",
    "품목_리스트 = ['건고추', '사과', '감자', '배', '깐마늘(국산)', '무', '상추', '배추', '양파', '대파']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7bf75c-6f17-46fe-a5f6-857131e27938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(raw_file, 산지공판장_file, 전국도매_file, 품목명, scaler=None):\n",
    "    raw_data = pd.read_csv(raw_file)\n",
    "    산지공판장 = pd.read_csv(산지공판장_file)\n",
    "    전국도매 = pd.read_csv(전국도매_file)\n",
    "\n",
    "    # 타겟 및 메타데이터 필터 조건 정의\n",
    "    conditions = {\n",
    "    '감자': {\n",
    "        'target': lambda df: (df['품종명'] == '감자 수미') & (df['거래단위'] == '20키로상자') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['감자'], '품종명': ['수미'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['감자'], '품종명': ['수미']}\n",
    "    },\n",
    "    '건고추': {\n",
    "        'target': lambda df: (df['품종명'] == '화건') & (df['거래단위'] == '30 kg') & (df['등급'] == '상품'),\n",
    "        '공판장': None, \n",
    "        '도매': None  \n",
    "    },\n",
    "    '깐마늘(국산)': {\n",
    "        'target': lambda df: (df['거래단위'] == '20 kg') & (df['등급'] == '상품'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['마늘'], '품종명': ['깐마늘'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['마늘'], '품종명': ['깐마늘']}\n",
    "    },\n",
    "    '대파': {\n",
    "        'target': lambda df: (df['품종명'] == '대파(일반)') & (df['거래단위'] == '1키로단') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['대파'], '품종명': ['대파(일반)'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['대파'], '품종명': ['대파(일반)']}\n",
    "    },\n",
    "    '무': {\n",
    "        'target': lambda df: (df['거래단위'] == '20키로상자') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['무'], '품종명': ['기타무'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['무'], '품종명': ['무']}\n",
    "    },\n",
    "    '배추': {\n",
    "        'target': lambda df: (df['거래단위'] == '10키로망대') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['배추'], '품종명': ['쌈배추'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['배추'], '품종명': ['배추']}\n",
    "    },\n",
    "    '사과': {\n",
    "        'target': lambda df: (df['품종명'].isin(['홍로', '후지'])) & (df['거래단위'] == '10 개') & (df['등급'] == '상품'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['사과'], '품종명': ['후지'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['사과'], '품종명': ['후지']}\n",
    "    },\n",
    "    '상추': {\n",
    "        'target': lambda df: (df['품종명'] == '청') & (df['거래단위'] == '100 g') & (df['등급'] == '상품'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['상추'], '품종명': ['청상추'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['상추'], '품종명': ['청상추']}\n",
    "    },\n",
    "    '양파': {\n",
    "        'target': lambda df: (df['품종명'] == '양파') & (df['거래단위'] == '1키로') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['양파'], '품종명': ['기타양파'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['양파'], '품종명': ['양파(일반)']}\n",
    "    },\n",
    "    '배': {\n",
    "        'target': lambda df: (df['품종명'] == '신고') & (df['거래단위'] == '10 개') & (df['등급'] == '상품'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['배'], '품종명': ['신고'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['배'], '품종명': ['신고']}\n",
    "    }\n",
    "    }\n",
    "\n",
    "    # 타겟 데이터 필터링\n",
    "    raw_품목 = raw_data[raw_data['품목명'] == 품목명]\n",
    "    target_mask = conditions[품목명]['target'](raw_품목)\n",
    "    filtered_data = raw_품목[target_mask]\n",
    "\n",
    "    # 다른 품종에 대한 파생변수 생성\n",
    "    other_data = raw_품목[~target_mask]\n",
    "    unique_combinations = other_data[['품종명', '거래단위', '등급']].drop_duplicates()\n",
    "    for _, row in unique_combinations.iterrows():\n",
    "        품종명, 거래단위, 등급 = row['품종명'], row['거래단위'], row['등급']\n",
    "        mask = (other_data['품종명'] == 품종명) & (other_data['거래단위'] == 거래단위) & (other_data['등급'] == 등급)\n",
    "        temp_df = other_data[mask]\n",
    "        for col in ['평년 평균가격(원)', '평균가격(원)']:\n",
    "            new_col_name = f'{품종명}_{거래단위}_{등급}_{col}'\n",
    "            filtered_data = filtered_data.merge(temp_df[['시점', col]], on='시점', how='left', suffixes=('', f'_{new_col_name}'))\n",
    "            filtered_data.rename(columns={f'{col}_{new_col_name}': new_col_name}, inplace=True)\n",
    "\n",
    "\n",
    "    # 공판장 데이터 처리\n",
    "    if conditions[품목명]['공판장']:\n",
    "        filtered_공판장 = 산지공판장\n",
    "        for key, value in conditions[품목명]['공판장'].items():\n",
    "            filtered_공판장 = filtered_공판장[filtered_공판장[key].isin(value)]\n",
    "        \n",
    "        filtered_공판장 = filtered_공판장.add_prefix('공판장_').rename(columns={'공판장_시점': '시점'})\n",
    "        filtered_data = filtered_data.merge(filtered_공판장, on='시점', how='left')\n",
    "\n",
    "    # 도매 데이터 처리\n",
    "    if conditions[품목명]['도매']:\n",
    "        filtered_도매 = 전국도매\n",
    "        for key, value in conditions[품목명]['도매'].items():\n",
    "            filtered_도매 = filtered_도매[filtered_도매[key].isin(value)]\n",
    "        \n",
    "        filtered_도매 = filtered_도매.add_prefix('도매_').rename(columns={'도매_시점': '시점'})\n",
    "        filtered_data = filtered_data.merge(filtered_도매, on='시점', how='left')\n",
    "\n",
    "    # 수치형 컬럼 처리\n",
    "    numeric_columns = filtered_data.select_dtypes(include=[np.number]).columns\n",
    "    filtered_data = filtered_data[['시점'] + list(numeric_columns)]\n",
    "    filtered_data[numeric_columns] = filtered_data[numeric_columns].fillna(0)\n",
    "\n",
    "    # # 정규화 적용\n",
    "    # if scaler is None:\n",
    "    #     scaler = MinMaxScaler()\n",
    "    #     filtered_data[numeric_columns] = scaler.fit_transform(filtered_data[numeric_columns])\n",
    "    # else:\n",
    "    #     filtered_data[numeric_columns] = scaler.transform(filtered_data[numeric_columns])\n",
    "\n",
    "    return filtered_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a979145-7788-48ad-955f-1a980ec4bb9c",
   "metadata": {},
   "source": [
    "# 베이지안 LSTM 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4286f03-f28b-4553-b605-51976971df44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베이지안 LSTM (From blitz)\n",
    "\n",
    "class BayesianLSTM(BayesianRNN):\n",
    "    \"\"\"\n",
    "    Bayesian LSTM layer, implements the linear layer proposed on Weight Uncertainity on Neural Networks\n",
    "    (Bayes by Backprop paper).\n",
    "\n",
    "    Its objective is be interactable with torch nn.Module API, being able even to be chained in nn.Sequential models with other non-this-lib layers\n",
    "    \n",
    "    parameters:\n",
    "        in_features: 입력 특성의 수 (정수)\n",
    "        out_features: 출력 특성의 수 (정수)\n",
    "        bias: 편향 항 사용 여부 (불리언)\n",
    "        prior_sigma_1: 혼합 사전 분포 1의 표준편차 (실수)\n",
    "        prior_sigma_2: 혼합 사전 분포 2의 표준편차 (실수)\n",
    "        prior_pi: 스케일된 혼합 사전 분포의 pi 값 (실수)\n",
    "        posterior_mu_init: 가중치 mu 초기화를 위한 사후 평균 (실수)\n",
    "        posterior_rho_init: 가중치 rho 초기화를 위한 사후 평균 (실수)\n",
    "        freeze: 가중치를 고정된(결정론적) 상태로 시작할지 여부 (불리언)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 out_features,\n",
    "                 bias = True,\n",
    "                 prior_sigma_1 = 0.1,\n",
    "                 prior_sigma_2 = 0.002,\n",
    "                 prior_pi = 1,\n",
    "                 posterior_mu_init = 0,\n",
    "                 posterior_rho_init = -6.0,\n",
    "                 freeze = False,\n",
    "                 prior_dist = None,\n",
    "                 peephole = False,\n",
    "                 **kwargs):\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = bias\n",
    "        self.freeze = freeze\n",
    "        self.peephole = peephole\n",
    "        \n",
    "        self.posterior_mu_init = posterior_mu_init\n",
    "        self.posterior_rho_init = posterior_rho_init\n",
    "        \n",
    "        self.prior_sigma_1 = prior_sigma_1\n",
    "        self.prior_sigma_2 = prior_sigma_2\n",
    "        self.prior_pi = prior_pi\n",
    "        self.prior_dist = prior_dist\n",
    "        \n",
    "        # Variational weight parameters and sample for weight ih\n",
    "        self.weight_ih_mu = nn.Parameter(torch.Tensor(in_features, out_features * 4).normal_(posterior_mu_init, 0.1))\n",
    "        self.weight_ih_rho = nn.Parameter(torch.Tensor(in_features, out_features * 4).normal_(posterior_rho_init, 0.1))\n",
    "        self.weight_ih_sampler = TrainableRandomDistribution(self.weight_ih_mu, self.weight_ih_rho)\n",
    "        self.weight_ih = None\n",
    "        \n",
    "        # Variational weight parameters and sample for weight hh\n",
    "        self.weight_hh_mu = nn.Parameter(torch.Tensor(out_features, out_features * 4).normal_(posterior_mu_init, 0.1))\n",
    "        self.weight_hh_rho = nn.Parameter(torch.Tensor(out_features, out_features * 4).normal_(posterior_rho_init, 0.1))\n",
    "        self.weight_hh_sampler = TrainableRandomDistribution(self.weight_hh_mu, self.weight_hh_rho)\n",
    "        self.weight_hh = None\n",
    "        \n",
    "        # Variational weight parameters and sample for bias\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(out_features * 4).normal_(posterior_mu_init, 0.1))\n",
    "        self.bias_rho = nn.Parameter(torch.Tensor(out_features * 4).normal_(posterior_rho_init, 0.1))\n",
    "        self.bias_sampler = TrainableRandomDistribution(self.bias_mu, self.bias_rho)\n",
    "        self.bias=None\n",
    "        \n",
    "        #our prior distributions\n",
    "        self.weight_ih_prior_dist = PriorWeightDistribution(self.prior_pi, self.prior_sigma_1, self.prior_sigma_2, dist = self.prior_dist)\n",
    "        self.weight_hh_prior_dist = PriorWeightDistribution(self.prior_pi, self.prior_sigma_1, self.prior_sigma_2, dist = self.prior_dist)\n",
    "        self.bias_prior_dist = PriorWeightDistribution(self.prior_pi, self.prior_sigma_1, self.prior_sigma_2, dist = self.prior_dist)\n",
    "        \n",
    "        self.init_sharpen_parameters()\n",
    "        \n",
    "        self.log_prior = 0\n",
    "        self.log_variational_posterior = 0\n",
    "    \n",
    "    \n",
    "    def sample_weights(self):\n",
    "        #sample weights\n",
    "        weight_ih = self.weight_ih_sampler.sample()\n",
    "        weight_hh = self.weight_hh_sampler.sample()\n",
    "        \n",
    "        #if use bias, we sample it, otherwise, we are using zeros\n",
    "        if self.use_bias:\n",
    "            b = self.bias_sampler.sample()\n",
    "            b_log_posterior = self.bias_sampler.log_posterior()\n",
    "            b_log_prior = self.bias_prior_dist.log_prior(b)\n",
    "            \n",
    "        else:\n",
    "            b = None\n",
    "            b_log_posterior = 0\n",
    "            b_log_prior = 0\n",
    "            \n",
    "        bias = b\n",
    "        \n",
    "        #gather weights variational posterior and prior likelihoods\n",
    "        self.log_variational_posterior = self.weight_hh_sampler.log_posterior() + b_log_posterior + self.weight_ih_sampler.log_posterior()\n",
    "        \n",
    "        self.log_prior = self.weight_ih_prior_dist.log_prior(weight_ih) + b_log_prior + self.weight_hh_prior_dist.log_prior(weight_hh)\n",
    "        \n",
    "        \n",
    "        self.ff_parameters = [weight_ih, weight_hh, bias]\n",
    "        return weight_ih, weight_hh, bias\n",
    "        \n",
    "    def get_frozen_weights(self):\n",
    "        \n",
    "        #get all deterministic weights\n",
    "        weight_ih = self.weight_ih_mu\n",
    "        weight_hh = self.weight_hh_mu\n",
    "        if self.use_bias:\n",
    "            bias = self.bias_mu\n",
    "        else:\n",
    "            bias = 0\n",
    "\n",
    "        return weight_ih, weight_hh, bias\n",
    "\n",
    "    \n",
    "    def forward_(self,\n",
    "                 x,\n",
    "                 hidden_states,\n",
    "                 sharpen_loss):\n",
    "        \n",
    "        if self.loss_to_sharpen is not None:\n",
    "            sharpen_loss = self.loss_to_sharpen\n",
    "            weight_ih, weight_hh, bias = self.sharpen_posterior(loss=sharpen_loss, input_shape=x.shape)\n",
    "        elif (sharpen_loss is not None):\n",
    "            sharpen_loss = sharpen_loss\n",
    "            weight_ih, weight_hh, bias = self.sharpen_posterior(loss=sharpen_loss, input_shape=x.shape)\n",
    "        \n",
    "        else:\n",
    "            weight_ih, weight_hh, bias = self.sample_weights()\n",
    "\n",
    "        #Assumes x is of shape (batch, sequence, feature)\n",
    "        bs, seq_sz, _ = x.size()\n",
    "        hidden_seq = []\n",
    "        \n",
    "        #if no hidden state, we are using zeros\n",
    "        if hidden_states is None:\n",
    "            h_t, c_t = (torch.zeros(bs, self.out_features).to(x.device), \n",
    "                        torch.zeros(bs, self.out_features).to(x.device))\n",
    "        else:\n",
    "            h_t, c_t = hidden_states\n",
    "        \n",
    "        #simplifying our out features, and hidden seq list\n",
    "        HS = self.out_features\n",
    "        hidden_seq = []\n",
    "        \n",
    "        for t in range(seq_sz):\n",
    "            x_t = x[:, t, :]\n",
    "            # batch the computations into a single matrix multiplication\n",
    "            \n",
    "            if self.peephole:\n",
    "                gates = x_t @ weight_ih + c_t @ weight_hh + bias\n",
    "            else:\n",
    "                gates = x_t @ weight_ih + h_t @ weight_hh + bias\n",
    "                g_t = torch.tanh(gates[:, HS*2:HS*3])\n",
    "            \n",
    "            i_t, f_t, o_t = (\n",
    "                torch.sigmoid(gates[:, :HS]), # input\n",
    "                torch.sigmoid(gates[:, HS:HS*2]), # forget\n",
    "                torch.sigmoid(gates[:, HS*3:]), # output\n",
    "            )\n",
    "            \n",
    "            if self.peephole:\n",
    "                c_t = f_t * c_t + i_t * torch.sigmoid(x_t @ weight_ih + bias)[:, HS*2:HS*3]\n",
    "                h_t = torch.tanh(o_t * c_t)\n",
    "            else:\n",
    "                c_t = f_t * c_t + i_t * g_t\n",
    "                h_t = o_t * torch.tanh(c_t)\n",
    "                \n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "            \n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "        \n",
    "        return hidden_seq, (h_t, c_t)\n",
    "\n",
    "    def forward_frozen(self,\n",
    "                       x,\n",
    "                       hidden_states):\n",
    "\n",
    "        weight_ih, weight_hh, bias = self.get_frozen_weights()\n",
    "\n",
    "        #Assumes x is of shape (batch, sequence, feature)\n",
    "        bs, seq_sz, _ = x.size()\n",
    "        hidden_seq = []\n",
    "        \n",
    "        #if no hidden state, we are using zeros\n",
    "        if hidden_states is None:\n",
    "            h_t, c_t = (torch.zeros(bs, self.out_features).to(x.device), \n",
    "                        torch.zeros(bs, self.out_features).to(x.device))\n",
    "        else:\n",
    "            h_t, c_t = hidden_states\n",
    "        \n",
    "        #simplifying our out features, and hidden seq list\n",
    "        HS = self.out_features\n",
    "        hidden_seq = []\n",
    "        \n",
    "        for t in range(seq_sz):\n",
    "            x_t = x[:, t, :]\n",
    "            # batch the computations into a single matrix multiplication\n",
    "            \n",
    "            if self.peephole:\n",
    "                gates = x_t @ weight_ih + c_t @ weight_hh + bias\n",
    "            else:\n",
    "                gates = x_t @ weight_ih + h_t @ weight_hh + bias\n",
    "                g_t = torch.tanh(gates[:, HS*2:HS*3])\n",
    "            \n",
    "            i_t, f_t, o_t = (\n",
    "                torch.sigmoid(gates[:, :HS]), # input\n",
    "                torch.sigmoid(gates[:, HS:HS*2]), # forget\n",
    "                torch.sigmoid(gates[:, HS*3:]), # output\n",
    "            )\n",
    "            \n",
    "            if self.peephole:\n",
    "                c_t = f_t * c_t + i_t * torch.sigmoid(x_t @ weight_ih + bias)[:, HS*2:HS*3]\n",
    "                h_t = torch.sigmoid(o_t * c_t)\n",
    "            else:\n",
    "                c_t = f_t * c_t + i_t * g_t\n",
    "                h_t = o_t * torch.tanh(c_t)\n",
    "                \n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "            \n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "        \n",
    "        return hidden_seq, (h_t, c_t)\n",
    "\n",
    "    def forward(self,\n",
    "                x,\n",
    "                hidden_states=None,\n",
    "                sharpen_loss=None):\n",
    "\n",
    "        if self.freeze:\n",
    "            return self.forward_frozen(x, hidden_states)\n",
    "        \n",
    "        if not self.sharpen:\n",
    "            sharpen_posterior = False\n",
    "            \n",
    "        return self.forward_(x, hidden_states, sharpen_loss)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85488f97-a865-4a21-a9f6-bddcb50a0e2a",
   "metadata": {},
   "source": [
    "# 모델 돌리기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a0b21c-2e84-40ae-bce6-2b03b6ff1f60",
   "metadata": {},
   "source": [
    "## 01. 데이터 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16ccbce-e0e5-485c-a2c8-5d3164b4b422",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_건고추 = process_data(\"../data/train/train.csv\", \n",
    "                         \"../data/train/meta/TRAIN_산지공판장_2018-2021.csv\", \n",
    "                         \"../data/train/meta/TRAIN_전국도매_2018-2021.csv\", \n",
    "                         '건고추')\n",
    "train_건고추"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595aeb38-e9e5-4011-a05b-17bc09b13385",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = train_건고추[\"평균가격(원)\"]\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "prices_arr = np.array(prices).reshape(-1, 1)\n",
    "prices = scaler.fit_transform(prices_arr)\n",
    "\n",
    "prices_unscaled = train_건고추[\"평균가격(원)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ce6718-32ab-4682-88fd-c76609a1a092",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 12\n",
    "\n",
    "def create_timestamps_ds(series, \n",
    "                         timestep_size=window_size):\n",
    "    time_stamps = []\n",
    "    labels = []\n",
    "    aux_deque = deque(maxlen=timestep_size)\n",
    "    \n",
    "    #starting the timestep deque\n",
    "    for i in range(timestep_size):\n",
    "        aux_deque.append(0)\n",
    "    \n",
    "    #feed the timestamps list\n",
    "    for i in range(len(series)-1):\n",
    "        aux_deque.append(series[i])\n",
    "        time_stamps.append(list(aux_deque))\n",
    "    \n",
    "    #feed the labels lsit\n",
    "    for i in range(len(series)-1):\n",
    "        labels.append(series[i + 1])\n",
    "    \n",
    "    assert len(time_stamps) == len(labels), \"Something went wrong\"\n",
    "    \n",
    "    #torch-tensoring it\n",
    "    features = torch.tensor(time_stamps[timestep_size:]).float()\n",
    "    labels = torch.tensor(labels[timestep_size:]).float()\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a219d50-c5cf-4829-9e5f-16c3287f8739",
   "metadata": {},
   "outputs": [],
   "source": [
    "@variational_estimator\n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__()\n",
    "        self.lstm_1 = BayesianLSTM(1, 10, prior_sigma_1=1, prior_pi=1, posterior_rho_init=-3.0)\n",
    "        self.linear = nn.Linear(10, 1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x_, _ = self.lstm_1(x)\n",
    "        \n",
    "        #gathering only the latent end-of-sequence for the linear layer\n",
    "        x_ = x_[:, -1, :]\n",
    "        x_ = self.linear(x_)\n",
    "        return x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4aa07e-cf2c-4894-831f-ed5fb2e5db1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs, ys = create_timestamps_ds(prices)\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs,\n",
    "                                                    ys,\n",
    "                                                    test_size=.25,\n",
    "                                                    random_state=42,\n",
    "                                                    shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "ds = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "dataloader_train = torch.utils.data.DataLoader(ds, batch_size=8, shuffle=True)\n",
    "\n",
    "net = NN()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707e9413-1f9b-468a-843d-b681604659ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 0\n",
    "for epoch in range(100):\n",
    "    for i, (datapoints, labels) in enumerate(dataloader_train):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = net.sample_elbo(inputs=datapoints,\n",
    "                               labels=labels,\n",
    "                               criterion=criterion,\n",
    "                               sample_nbr=3,\n",
    "                               complexity_cost_weight=1/X_train.shape[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        iteration += 1\n",
    "        if iteration%250==0:\n",
    "            preds_test = net(X_test)[:,0].unsqueeze(1)\n",
    "            loss_test = criterion(preds_test, y_test)\n",
    "            print(\"Iteration: {} Val-loss: {:.4f}\".format(str(iteration), loss_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35a54a6-d645-4195-9728-688fc7a5e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_unscaled\n",
    "original = prices_unscaled [1:][window_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeb9011-8a2c-43ba-8c97-9d5150572ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f5b75d-6d72-4c55-8d65-e7cd1c885b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame(original)\n",
    "df_pred[\"Date\"] = train_건고추['시점']\n",
    "df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e64535-4182-4041-8868-d4840fbc51c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_price_future(X_test, future_length, sample_nbr=10):\n",
    "    \n",
    "    #sorry for that, window_size is a global variable, and so are X_train and Xs\n",
    "    global window_size\n",
    "    global X_train\n",
    "    global Xs\n",
    "    global scaler\n",
    "    \n",
    "    #creating auxiliar variables for future prediction\n",
    "    preds_test = []\n",
    "    test_begin = X_test[0:1, :, :]\n",
    "    test_deque = deque(test_begin[0,:,0].tolist(), maxlen=window_size)\n",
    "\n",
    "    idx_pred = np.arange(len(X_train), len(Xs))\n",
    "    \n",
    "    #predict it and append to list\n",
    "    for i in range(len(X_test)):\n",
    "        #print(i)\n",
    "        as_net_input = torch.tensor(test_deque).unsqueeze(0).unsqueeze(2)\n",
    "        pred = [net(as_net_input).cpu().item() for i in range(sample_nbr)]\n",
    "        \n",
    "        \n",
    "        test_deque.append(torch.tensor(pred).mean().cpu().item())\n",
    "        preds_test.append(pred)\n",
    "        \n",
    "        if i % future_length == 0:\n",
    "            #our inptus become the i index of our X_test\n",
    "            #That tweak just helps us with shape issues\n",
    "            test_begin = X_test[i:i+1, :, :]\n",
    "            test_deque = deque(test_begin[0,:,0].tolist(), maxlen=window_size)\n",
    "\n",
    "    #preds_test = np.array(preds_test).reshape(-1, 1)\n",
    "    #preds_test_unscaled = scaler.inverse_transform(preds_test)\n",
    "    \n",
    "    return idx_pred, preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ada138b-67fe-4151-aff7-772b30f263b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confidence_intervals(preds_test, ci_multiplier):\n",
    "    global scaler\n",
    "    \n",
    "    preds_test = torch.tensor(preds_test)\n",
    "    \n",
    "    pred_mean = preds_test.mean(1)\n",
    "    pred_std = preds_test.std(1).detach().cpu().numpy()\n",
    "\n",
    "    pred_std = torch.tensor((pred_std))\n",
    "    #print(pred_std)\n",
    "    \n",
    "    upper_bound = pred_mean + (pred_std * ci_multiplier)\n",
    "    lower_bound = pred_mean - (pred_std * ci_multiplier)\n",
    "    #gather unscaled confidence intervals\n",
    "\n",
    "    pred_mean_final = pred_mean.unsqueeze(1).detach().cpu().numpy()\n",
    "    pred_mean_unscaled = scaler.inverse_transform(pred_mean_final)\n",
    "\n",
    "    upper_bound_unscaled = upper_bound.unsqueeze(1).detach().cpu().numpy()\n",
    "    upper_bound_unscaled = scaler.inverse_transform(upper_bound_unscaled)\n",
    "    \n",
    "    lower_bound_unscaled = lower_bound.unsqueeze(1).detach().cpu().numpy()\n",
    "    lower_bound_unscaled = scaler.inverse_transform(lower_bound_unscaled)\n",
    "    \n",
    "    return pred_mean_unscaled, upper_bound_unscaled, lower_bound_unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a58ae1-7b07-4053-a17c-5ffbd0fb6357",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_length=8\n",
    "sample_nbr=4\n",
    "ci_multiplier=5\n",
    "idx_pred, preds_test = pred_price_future(X_test, future_length, sample_nbr)\n",
    "pred_mean_unscaled, upper_bound_unscaled, lower_bound_unscaled = get_confidence_intervals(preds_test,\n",
    "                                                                                          ci_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791c9f0c-66c1-460f-b36f-fe269e5247b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(train_건고추['평균가격(원)'][113:]).reshape(-1, 1)\n",
    "under_upper = upper_bound_unscaled > y\n",
    "over_lower = lower_bound_unscaled < y\n",
    "total = (under_upper == over_lower)\n",
    "\n",
    "print(\"{} our predictions are in our confidence interval\".format(np.mean(total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07ab2ac-ec46-4434-bfe4-ea32c114654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_pred = idx_pred + window_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e27d7e-3081-4292-ba70-9141db2e616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"ytick.color\" : \"w\",\n",
    "          \"xtick.color\" : \"w\",\n",
    "          \"axes.labelcolor\" : \"w\",\n",
    "          \"axes.edgecolor\" : \"w\"}\n",
    "\n",
    "plt.rcParams.update(params)\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "\n",
    "plt.title(\"Agriculture prices\", color=\"white\")\n",
    "\n",
    "plt.plot(df_pred.index,\n",
    "         df_pred['평균가격(원)'],\n",
    "         color='black',\n",
    "         label=\"Real\")\n",
    "\n",
    "plt.plot(idx_pred,\n",
    "         pred_mean_unscaled,\n",
    "         label=\"Prediction for {} days, than consult\".format(future_length),\n",
    "         color=\"red\")\n",
    "\n",
    "plt.fill_between(x=idx_pred,\n",
    "                 y1=upper_bound_unscaled[:,0],\n",
    "                 y2=lower_bound_unscaled[:,0],\n",
    "                 facecolor='green',\n",
    "                 label=\"Confidence interval\",\n",
    "                 alpha=0.5)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb32e0b4-fe64-4cfe-862d-b39239be6212",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"ytick.color\" : \"w\",\n",
    "          \"xtick.color\" : \"w\",\n",
    "          \"axes.labelcolor\" : \"w\",\n",
    "          \"axes.edgecolor\" : \"w\"}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "plt.title(\"IBM Stock prices\", color=\"white\")\n",
    "\n",
    "\n",
    "plt.fill_between(x=idx_pred,\n",
    "                 y1=upper_bound_unscaled[:,0],\n",
    "                 y2=lower_bound_unscaled[:,0],\n",
    "                 facecolor='green',\n",
    "                 label=\"Confidence interval\",\n",
    "                 alpha=0.75)\n",
    "\n",
    "plt.plot(idx_pred,\n",
    "         df_pred.Close[-len(pred_mean_unscaled):],\n",
    "         label=\"Real\",\n",
    "         alpha=1,\n",
    "         color='black',\n",
    "         linewidth=0.5)\n",
    "\n",
    "plt.plot(idx_pred,\n",
    "         pred_mean_unscaled,\n",
    "         label=\"Prediction for {} days, than consult\".format(future_length),\n",
    "         color=\"red\",\n",
    "         alpha=0.5)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6635f704-6f2d-4ef2-b5eb-1a71b3e84b14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
